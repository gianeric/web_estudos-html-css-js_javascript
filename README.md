# ENGLISH INSIGHT GEN AI

**Projeto pessoal para estudos:** corretor de frases em ingl√™s usando **Python** (backend) e **JavaScript** (frontend), integrado ao **Google Gemini AI**.

---

## üõ† Tecnologias

[![Python](https://img.shields.io/badge/Python-3.12-blue?logo=python&logoColor=white)](https://www.python.org/)  
[![JavaScript](https://img.shields.io/badge/JavaScript-ES6-yellow?logo=javascript&logoColor=black)](https://developer.mozilla.org/en-US/docs/Web/JavaScript)  
[![Gemini AI](https://img.shields.io/badge/GeminiAI-Google-lightgrey)](https://cloud.google.com/vertex-ai)  
[![AWS](https://img.shields.io/badge/AWS-Serverless-orange?logo=amazon-aws&logoColor=white)](https://aws.amazon.com/)  
[![AWS SAM CLI](https://img.shields.io/badge/AWS%20SAM-CLI-red?logo=amazon-aws&logoColor=white)](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/what-is-sam.html)

---

## üöÄ Rodando o backend localmente

### 1Ô∏è‚É£ Clone o projeto

```bash
git clone https://github.com/gianeric/english-insight-gen-ai.git
cd english-insight-gen-ai/backend
```

### 2Ô∏è‚É£ Iniciar o LocalStack (Docker)

LocalStack simula os servi√ßos da AWS localmente.

```bash
cd backend/docker
docker-compose up -d
```

Refer√™ncia: [LocalStack GitHub](https://github.com/localstack/localstack)

### 3Ô∏è‚É£ Configurar vari√°veis de ambiente para o GEMINI AI

No diret√≥rio backend/app crie o arquivo env.json (n√£o commit√°-lo):

```json
{
  "ApiLambda": {
    "GEMINI_API_KEY": "SUA_CHAVE_AQUI"
  }
}
```

üí° Dica: Esse arquivo ser√° usado pelo SAM para injetar a chave da sua conta Gemini na Lambda durante o desenvolvimento local.

### 4Ô∏è‚É£ Instalar AWS SAM CLI (Windows)

```bash
choco install awssamcli
```

### 5Ô∏è‚É£ Compilar a Lambda

No diret√≥rio backend/app:

```bash
sam build
```

### 6Ô∏è‚É£ Rodar a Lambda localmente

```bash
sam local start-api --env-vars env.json
```

O endpoint ficar√° dispon√≠vel em:

```bash
http://localhost:3000/corrector
```

### 7Ô∏è‚É£ Testando a API

Voc√™ pode usar o Postman, Insomnia ou qualquer outra ferramenta HTTP.

M√©todo: POST
URL: http://localhost:3000/corrector
Body (JSON):

```json
{
  "sentence": "He don't like the movie."
}
```

Resposta esperada:

```json
{
  "correction": "Correct: He doesn't like the movie. Explica√ß√£o curta: Para a terceira pessoa do singular (he, she, it), usa-se 'doesn't' (does not) na forma negativa do Simple Present, n√£o 'don't'."
}
```

### 8Ô∏è‚É£ Debug no VSCode

Se quiser depurar a Lambda localmente com VSCode:

No diret√≥rio backend/app, crie o arquivo .env (n√£o commit√°-lo):

```bash
GEMINI_API_KEY=SUA_CHAVE_AQUI
```

Descomente esse trecho de c√≥digo em backend\app\lambda_function.py

```python
if __name__ == "__main__":
    setence = {'body': '{\n\t"sentence": "She don\'t like the movie."\n}'}
    handler(setence, "context")
```

Utilize o launch.json do VSCode em ".vscode\lauch.json"

Assim, voc√™ consegue depurar a Lambda como se estivesse rodando localmente, podendo inspecionar vari√°veis e logs.

---

üí° Dicas

Sempre mantenha sua chave do Gemini fora do GitHub.

LocalStack √© opcional, mas ajuda a simular servi√ßos AWS sem gastar cr√©ditos.

Timeout para chamadas √† API Gemini pode variar; configure seu template.yml com um tempo maior (ex: 30s).
